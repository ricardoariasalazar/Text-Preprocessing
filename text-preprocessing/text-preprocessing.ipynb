{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Text Preprocessing \n",
    "**Date:** 09/09/2020                              \n",
    "Version: 1.0\n",
    "\n",
    "## 1. Introduction\n",
    "Convert a set of tweets and convert them into numerical representations which are suitable for input into recommender-systems and information-retrieval algorithms. \n",
    "\n",
    "\n",
    "## 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Get file from a particular path\n",
    "import re # Regular expressions\n",
    "import langid # Filtering enlish tweets\n",
    "import pandas as pd # Handle data frames and import Excel files\n",
    "import nltk # Text processing\n",
    "from nltk.tokenize import RegexpTokenizer # Tokenizer\n",
    "from nltk.tokenize import MWETokenizer # Tokenize multiple words\n",
    "from nltk.collocations import * # Bigrams collocations\n",
    "from itertools import chain # Apprend many lists together\n",
    "import itertools \n",
    "from nltk.stem import PorterStemmer # Stem\n",
    "from nltk.util import ngrams # Get ngrams out of a list of text\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Create a count vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Pre-Processing\n",
    "\n",
    "\n",
    "### 1. Examining and loading data\n",
    "The first step is to load the excel file given (30550971.xlsx), that is stored in a folder called part2 that is in the same location of this notebook. We get the information of this file by using the fuction ExcelFile that belongs to Pandas library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.excel._base.ExcelFile at 0x19ea3bacd00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_data = pd.ExcelFile(os.path.join(os.getcwd(), 'data/tweets.xlsx')) # Get the information stored in the excel file\n",
    "excel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file imported was stored as a diference Data Frames as each sheet of the excel file, has a different data frame to be parsed. As we can see in the following example:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Excel File has 81 sheets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2020-03-22', '2020-03-23', '2020-03-24', '2020-03-25', '2020-03-26']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheets = excel_data.sheet_names # Get a list with all the names of the sheets of the excel file\n",
    "print('The Excel File has ' + str(len(sheets)) + ' sheets')\n",
    "sheets[:5] # Names of the first 5 sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context independent words are those that are used in every text no matter what its topic is. That is why we have to get rid of these words. There are some libraries in Python that have the list of the Context Independent words, but in this case we are going to use the .txt file given and store the information in it in a list called `ci_stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', \"a's\", 'able', 'about', 'above']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ci_stopwords = []\n",
    "with open(os.path.join(os.getcwd(), 'part2/stopwords_en.txt'), 'r', encoding=\"utf8\") as file: # Open the file \n",
    "    for line in file.readlines(): # Read each word that is stored in each line of the file\n",
    "        ci_stopwords.append(line.strip()) # Strip the word in each line and append it to the list\n",
    "        \n",
    "ci_stopwords[:5] # Firts 5 context independent stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Parsing the data\n",
    "After we manage to extract the information of each sheet of the Excel File, we have to parse it to get a clean data frame with the required information. To do this we have to:\n",
    "- **Drop useless columns**: Columns where all the values are NA\n",
    "- **Drop useless rows**: Rows where all the values are NA\n",
    "- **Fix the header**: Make an uniform header for all the data frames\n",
    "\n",
    "After we make this corrections to we are going to store all the tweets in the same list, we are going to do this by extracting the column **text** in every data frame and append it to a dictionary called `tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = {} # List that will store all the tweets given\n",
    "for sheet in sheets: # Just do this to the first sheet as a demonstration\n",
    "    df = excel_data.parse(sheet) # Parse the sheet to get a dataframe with the information\n",
    "    df.dropna(axis = 0, how='all', inplace=True) # Drop all the rows where the value of 'all' cells is NaN\n",
    "    df.dropna(axis = 1, how='all', inplace=True) # Drop all the columns where the value of 'all' cells is NaN\n",
    "    df.index = range(len(df.index)) # As we drop some rows, we restart the indexes from 0\n",
    "    header = [df.columns[0], df.columns[1], df.columns[2]] # Define the headers as the first row of df\n",
    "\n",
    "    if header != ['text', 'id', 'created_at']: # If the header has not this structure\n",
    "        df.rename(columns=df.iloc[0], inplace = True) # Define the first row as the header of the df\n",
    "        df.drop(df.index[0], inplace=True) # Drop the first row, as it is the header\n",
    "    \n",
    "    text = df['text'].tolist() # Convert the text column to a list\n",
    "    tweets[sheet] = text # Append the list with the tweets of a sheets to the list that will store all the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filtering English Tweets\n",
    "By using the library 'Langid' we can filter out the non-english tweets, as we are only interested in those that are written in English. We have to identify the language of each tweet and then classify it. The final result is a smaller dictionary with only the english tweet that will be called `en_tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tweets = {} # Dictionary to store the english tweets\n",
    "\n",
    "for date in tweets: # For every date in tweets\n",
    "    text = [] # Empty list of tweets. To store all the english tweets written in each day\n",
    "    \n",
    "    for tweet in tweets[date]: # For each tweet in the tweets list\n",
    "        if langid.classify(str(tweet))[0] == 'en': # If the tweet is written in english\n",
    "            text.append(tweet)\n",
    "    \n",
    "    en_tweets[date] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Word Tokenization\n",
    "Now, we need to convert each tweet to lower case to then tokenize each tweet in **en_tweets** with the following Regular Expression: **[a-zA-Z]+(?:[-'][a-zA-Z]+)?**. And then store all the unique tokens in a dictionary called *tokens_tweets*, where the keys are the dates and the values are the list of the words of that date.\n",
    "\n",
    "<img src=\"./images/tokenization.png\" width=\"500\" height=\"250\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\") # Regular expression used to get the tokens       \n",
    "tokens_tweets = {} # Dictionary to store the tokens per date\n",
    "\n",
    "for date in en_tweets: # For every date in tweets\n",
    "    tokens = [] # List to store the tokens\n",
    "    \n",
    "    for tweet in en_tweets[date]: # For each tweet in the en_tweets list\n",
    "        tokens = tokens + tokenizer.tokenize(str(tweet).lower()) # Tokenize each tweet and convert each token to lower case\n",
    "    \n",
    "    tokens_tweets[date] = tokens # Store all the tokens of a date in the dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate list of bigrams\n",
    "Before removing useless words or stemming the words we have to create the bigrams, therefore we use the library **nltk.collocations** to get the first 200 bigram collocations by using PMI measure, given the unigrams of all tweets.\n",
    "\n",
    "First we concatenate all the daiy unigrams, in order to create a huge unique list with all the tokens of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = list(chain.from_iterable([token for token in tokens_tweets.values()])) # Get all the unigrams of the tweet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, with the list that we just create we build a new list with the top 200 bigrams measured by using PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(unigrams) \n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-200 bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, as the function **nltk.collocations.nbest(bigram_measures.pmi, n)** returns a list with sets of the bigrams. We proceed to convert this sets to the following format:\n",
    "\n",
    "$$\\text{set[0]_set[1]}$$\n",
    "\n",
    "And append this value to a list called bigrams. So at the end the list **bigrams** will store the top 200 bigrams with the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in top_200_bigrams:\n",
    "    bigram = str(i[0].strip()) + '_' + str(i[1].strip())\n",
    "    bigrams.append(bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Removal of useless words\n",
    "Now that we already create the bigram list, we can handle the unigrams list to performe the following steps:\n",
    "\n",
    "#### 6.1 Remove of duplicate words\n",
    "As we are constructing the vocabulary of the tweets, we can get rid of duplicate values. We do this by converting the list unigrams to a set. And then we convert the set again to a list to make it easier to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = list(set(unigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Remove Context Independent Stopwords\n",
    "There some words in all the language that are useful to construct correct sentences, these words are often functional words for example, articles, pronouns, particles, and so on. \n",
    "There are some libraries in Python that have these words compiled, but in this case we are going to use the words in the file **stopwords_en.txt** given to do this assigment and was already imported in the first step. The Context Independent Stop Words are stored in a list called **ci_stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [x for x in unigrams if x not in set(ci_stopwords)] # Remove context independent stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Remove unigrams with less than 3 letters\n",
    "In order to get words we some meaningful significant, we get rid of words with less that 3 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [x for x in unigrams if 3 <=  len(x)] # Tokens with less than 3 letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. Remove words due its frequency\n",
    "By getting the frequency of occurrence of some words distributed along the documents, we can identify two different type of words.\n",
    "\n",
    "- **Rare tokens:** Words that occur in less than 5 documents (days)\n",
    "- **Context dependent stop words:** Words that occur in more than 60 documents (days)\n",
    "\n",
    "First, we are going to find the frequency distribution along with the documents of the unigrams. By getting the unique values of the unigrams of each day and then append these unigrams together. And then, we count the frequency of each word, returning the occurrence along with the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_unigrams = list(chain.from_iterable([set(token) for token in tokens_tweets.values()])) \n",
    "freq_dist = nltk.FreqDist(daily_unigrams) # Get the frequency distribution of the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.4.1. Remove of Rare Tokens\n",
    "As we said before, rare tokens are the words that have a frequency distribution of less than 5. So first we construct the list with this rare tokens, and finally remove them from the list unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_tokens = [x for x in freq_dist.keys() if freq_dist.get(x) < 5] # Rare tokens\n",
    "\n",
    "unigrams = [x for x in unigrams if x not in set(rare_tokens)] # Remove context dependent stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.4.2. Remove of Context Dependent Stop Words\n",
    "As we said before, Context Dependent Stop Words are the words that have a frequency distribution of more than 60. So first we construct the list with this Context Dependent Stop Words, and finally remove them from the list unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_stopwords = [x for x in freq_dist.keys() if freq_dist.get(x) > 60] # Context dependent stop words\n",
    "\n",
    "unigrams = list(set(unigrams) - set(cd_stopwords))  # Remove context dependent stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Stemming of tokens\n",
    "Stemming the action of reducing different words into the 'root' same word. In English, nouns are inflected in the plural, verbs are inflected in the various tenses, and adjectives are inflected in the comparative/superlative. So the idea is to just keep this 'root' word instead of having to many different forms of the same word.\n",
    "We do this by using the function `PortStemmer` and apply the stem to all words in the list unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() # Porter Stemmer\n",
    "unigrams = ['{1}'.format(w, stemmer.stem(w)) for w in unigrams] # Covert all the elements in unigram to a stem form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Create the Vocabulary\n",
    "Now that we have the unigrams without the useless words and stemmed, and the top 200 bigrams. We proceed to create the sample vocabulary with these two lists.\n",
    "\n",
    "So that, we append the lists unigrams and bigrams, then we sort the new list in alphabetical order, in order to get a new list called `vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = unigrams + bigrams\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we inspect the values of the `vocab` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a-chinese_bioweapon-greatawakening',\n",
       " 'aaa',\n",
       " 'aaaaaaa_comel',\n",
       " 'aaaahjhvas_chaitye',\n",
       " 'aaak_thooooo']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:5] # First 5 words of the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Daily Frequency Distribution of Unigrams\n",
    "In this task we proceed to create distionary called `freq_unigrams`. The keys of the dictionary are going to be the dates and the values are a list of the top 100 unigrams per day.\n",
    "\n",
    "What we do here is to get the original unigrams per day, stem all the words and only get the words that are in the list **unigrams** that have already removed the useless words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_unigrams = {}\n",
    "for date in tokens_tweets:\n",
    "    daily_unigrams = ['{1}'.format(x, stemmer.stem(x)) for x in tokens_tweets[date]]\n",
    "    daily_unigrams = [x for x in daily_unigrams if x in set(unigrams)]\n",
    "    freq_unigrams[date] = nltk.FreqDist(daily_unigrams).most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we inspect the values of the `freq_unigrams` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('co', 927), ('in', 327), ('thi', 209), ('be', 145), ('on', 130)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_unigrams['2020-03-22'][:5] # First 5 unigrams of the date 2020-03-22 with its frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Daily Frequency Distribution of Bigrams\n",
    "In this task we proceed to create distionary called `freq_bigrams`. The keys of the dictionary are going to be the dates and the values are a list of the top 100 bigrams per day.\n",
    "\n",
    "What we do here is to get the original unigrams per day, create bigrams out of these lists and get the top 100 of each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bigrams = {}\n",
    "for date in tokens_tweets:\n",
    "    daily_bigrams = ngrams(tokens_tweets[date], n=2)\n",
    "    freq_bigrams[date] = nltk.FreqDist(daily_bigrams).most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we inspect the values of the `freq_bigrams` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('https', 't'), 925),\n",
       " (('t', 'co'), 925),\n",
       " (('the', 'coronavirus'), 64),\n",
       " (('coronavirus', 'https'), 56),\n",
       " (('of', 'the'), 47)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams['2020-03-22'][:5] # First 5 unigrams of the date 2020-03-22 with its frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Create the .txt files\n",
    "\n",
    "#### 11.1. vocabulary.txt\n",
    "In this file we are going to save the unigrams and bigrams with the format of sample_vocab.txt file. The data is going to be extracted from the list `vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_txt = open('vocabulary.txt', 'w', encoding='utf8') # Create a new .txt file\n",
    "\n",
    "for token in vocab: # For every token in vocab list\n",
    "    text = str(token) + ':' + str(vocab.index(token)) + '\\n' # Format -> token:id\n",
    "    vocab_txt.write(text) # Write the text in the file 30550971_vocab.txt\n",
    "    \n",
    "vocab_txt.close() # Close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2. 100unigrams.txt\n",
    "In this file we are going to save the top 100 unigrams of each date of the file with the format of sample_100uni.txt file. The data is going to be extracted from the dictionary `freq_unigrams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni100_txt = open('100unigrams.txt', 'w', encoding='utf8') # Create a new .txt file\n",
    "\n",
    "for date in freq_unigrams: # For every token in vocab list\n",
    "    text = str(date) + ':' + str(freq_unigrams[date]) + '\\n' # Format -> token:FreqDist\n",
    "    uni100_txt.write(text) # Write the text in the file 30550971_100uni.txt\n",
    "    \n",
    "uni100_txt.close() # Close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.3. 100unigrams.txt\n",
    "In this file we are going to save the top 100 bigrams of each date of the file with the format of sample_100bi.txt file. The data is going to be extracted from the dictionary `freq_bigrams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi100_txt = open('100bigrams.txt', 'w', encoding='utf8') # Create a new .txt file\n",
    "\n",
    "for date in freq_bigrams: # For every token in vocab list\n",
    "    text = str(date) + ':' + str(freq_bigrams[date]) + '\\n' # Format -> token:FreqDist\n",
    "    bi100_txt.write(text) # Write the text in the file 30550971_100bi.txt\n",
    "    \n",
    "bi100_txt.close() # Close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Open files with os library](https://stackoverflow.com/questions/18262293/how-to-open-every-file-in-a-folder)\n",
    "\n",
    "[Classify english tweets](https://stackoverflow.com/questions/39142778/python-how-to-determine-the-language)\n",
    "\n",
    "[Column to list](https://stackoverflow.com/questions/22341271/get-list-from-pandas-dataframe-column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
